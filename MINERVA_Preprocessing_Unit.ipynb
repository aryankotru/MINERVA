{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aryankotru/MINERVA/blob/min_beta/MINERVA_Preprocessing_Unit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "84mv7Y58BapE"
      },
      "outputs": [],
      "source": [
        "import scipy.io\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pywt\n",
        "import math\n",
        "import csv\n",
        "import os\n",
        "from collections import Counter\n",
        "from scipy.stats import entropy, kurtosis, skew\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from scipy.signal import butter, sosfilt, sosfreqz, lfilter, iirnotch, spectrogram\n",
        "from google.colab import drive\n",
        "\n",
        "#TODO:\n",
        "#test with GUI in real-time\n",
        "#motor imagery data\n",
        "#collect a lot more data for each predictor\n",
        "#ssvep binary classification\n",
        "#motor imagery - left foot, right hand\n",
        "#more data, less predictors\n",
        "#siamese network - one shot learning\n",
        "#fix shannon entro and other features"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_input_file(input_file):\n",
        "    data = []\n",
        "    with open(input_file, 'r') as file:\n",
        "        for line in file:\n",
        "            line = line.strip()\n",
        "            if line.startswith('%Sample Rate'):\n",
        "                key_value = line[1:].strip().split(' = ')\n",
        "                value = key_value[1]\n",
        "                fs = int(value.strip().split(' ')[0])\n",
        "\n",
        "            elif line.startswith('Sample Index'):\n",
        "                header = line.split(', ')\n",
        "            else:\n",
        "                data.append(line.split(', ')) #typecast as int'\n",
        "\n",
        "    data = [sublist[1:][:8] for sublist in data[3:]]\n",
        "    converted_array = np.array(data, dtype=float).tolist()\n",
        "    return fs, header[:9], converted_array\n",
        "\n",
        "def convert_and_write_output(output_file, header, data):\n",
        "    with open(output_file, 'w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "\n",
        "        # Write metadata\n",
        "        #writer.writerow(['% Metadata'])\n",
        "        \"\"\"for key, value in metadata.items():\n",
        "          if key == 'Sample Rate':\n",
        "            fs = int(value.strip().split(' ')[0])\"\"\"\n",
        "            #print(fs)\n",
        "            #writer.writerow([f'% {key} = {value}'])\n",
        "\n",
        "        # Write data header\n",
        "        #writer.writerow(['% Data'])\n",
        "        \"\"\"update_header = []\n",
        "        for header_val in header:\n",
        "          if header_val.startswith('EXG Channel'):\n",
        "            header_val = header_val[:].split('EXG Channel ')\n",
        "            update_header.append(int(header_val[1]))\n",
        "        writer.writerow(update_header[:])\"\"\"\n",
        "\n",
        "        # Write data rows\n",
        "         #removes index and keeps only 8 channel data from OpenBCI format\n",
        "        writer.writerows(data)\n",
        "    return fs, data\n",
        "\n",
        "\n",
        "\n",
        "def get_latest_subdirs(b):\n",
        "  result = []\n",
        "  for d in os.listdir(b):\n",
        "    bd = os.path.join(b, d)\n",
        "    if os.path.isdir(bd): result.append(bd)\n",
        "    latest_subdir = max(result, key=os.path.getmtime)\n",
        "  return latest_subdir\n",
        "\n",
        "\n",
        "def get_input_raw_file(latest_subdir):\n",
        "  file_names = []\n",
        "  for x in os.listdir(latest_subdir):\n",
        "    if x.endswith(\".txt\"):\n",
        "      filepath = os.path.join(latest_subdir, x)\n",
        "    else:\n",
        "      return \"No matching file\"\n",
        "  return filepath\n"
      ],
      "metadata": {
        "id": "1Mk7NM1XTH8n"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3uyeMIB3-lY"
      },
      "source": [
        "global variable declaration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Vzg0v3bq358C"
      },
      "outputs": [],
      "source": [
        "epoch = 710 #calc epoch len from fs and epoch time len\n",
        "num_epochs = 12\n",
        "# Sample rate and desired cutoff frequencies (in Hz)*\n",
        "fs = 250\n",
        "fs = fs\n",
        "lowcut = 1\n",
        "highcut = 30\n",
        "filter_order = 9\n",
        "notch_quality_factor = 0.693\n",
        "wavelet = 'sym9'\n",
        "drive_mounted = False\n",
        "\n",
        "freq_bands = {\"delta\": [0.3, 4], \"theta\": [4, 8], \"alpha\": [8, 13], \"beta\": [13, 30], \"gamma\": [30, 50]}\n",
        "chars = [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"0\", \"*\", \"#\"]\n",
        "targets = [1 , 2 ,3 , 4, 5, 6,7,8,9,10,11,12]\n",
        "feature_names = ['Mean', 'Standard Deviation', 'Skewness', 'Entropy', 'Energy', 'Kurtosis', 'Median' ,'Variance']\n",
        "\n",
        "channel_names = [\"POz\", \"PO3\", \"PO4\", \"PO5\", \"PO6\", \"Oz\", \"O1\", \"O2\"]\n",
        "channel_numbers = [1,2,3,4,5,6,7,8]\n",
        "\n",
        "T =  2.84\n",
        "nsamples = T * fs\n",
        "t = np.arange(0, epoch) / fs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1wwy0mEsFip"
      },
      "source": [
        "data filtering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "T3aQmMENiHap"
      },
      "outputs": [],
      "source": [
        "def notch_filter(data, rate, freq, quality):\n",
        "      x = scipy.signal.filtfilt(*scipy.signal.iirnotch(freq / (rate / 2), quality), data)\n",
        "      #https://neuraldatascience.io/7-eeg/erp_filtering.html\n",
        "      return x\n",
        "\n",
        "def butter_bandpass(lowcut, highcut, fs, order=filter_order):\n",
        "        nyq = 0.5 * fs #\n",
        "        low = lowcut / nyq\n",
        "        high = highcut / nyq\n",
        "        sos = butter(order, [low, high], analog=False, btype='band', output='sos')\n",
        "        return sos\n",
        "\n",
        "def butter_bandpass_filter(data, lowcut, highcut, fs, order=filter_order):\n",
        "        sos = butter_bandpass(lowcut, highcut, fs, order=order)\n",
        "        y = sosfilt(sos, data)\n",
        "        return y"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "data normalization"
      ],
      "metadata": {
        "id": "ihVd8UWUrwLN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dataset_scaler(dataframe, column_names):\n",
        "  scaler = MinMaxScaler() #normalizing data points, object of class StandardScaler\n",
        "  minmax_scaled_data = scaler.fit_transform((dataframe))\n",
        "  np.reshape(minmax_scaled_data, dataframe.shape)\n",
        "  scaled_data = np.vstack((minmax_scaled_data))\n",
        "  dfa = pd.DataFrame(minmax_scaled_data)\n",
        "  dfa.columns = column_names #index for columns\n",
        "  return dfa"
      ],
      "metadata": {
        "id": "iSHIUHkLbrPV"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eta(data, unit='shannon'):\n",
        "    base = {\n",
        "        'shannon' : 2.,\n",
        "        'natural' : math.exp(1),\n",
        "        'hartley' : 10.}\n",
        "    if len(data) <= 1:\n",
        "        return 0\n",
        "    counts = Counter()\n",
        "    for d in data:\n",
        "        counts[d] += 1\n",
        "    ent = 0\n",
        "    probs = [round(float(c)) / len(data) for c in counts.values()]\n",
        "    for p in probs:\n",
        "        if p > 0.:\n",
        "            ent -= p * math.log(p, base[unit])\n",
        "    return ent\n",
        "  #https://stackoverflow.com/questions/15450192/fastest-way-to-compute-entropy-in-python\n",
        "\n",
        "def calculate_features(coeffs):\n",
        "    mean_val = np.mean(coeffs)\n",
        "    std_val = np.std(coeffs) #maybe try variance\n",
        "    skewness_val = skew(coeffs)\n",
        "    entropy_val = eta(coeffs)\n",
        "    energy = np.sum(coeffs**2)\n",
        "    kurtosis_value = kurtosis(coeffs)\n",
        "    median_value = np.median(coeffs)\n",
        "    variance_value = np.var(coeffs)\n",
        "    return mean_val, std_val, skewness_val, entropy_val, energy, kurtosis_value, median_value, variance_value"
      ],
      "metadata": {
        "id": "e34dY9IAOSPc"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "wavelet transform & feature extraction"
      ],
      "metadata": {
        "id": "GEaa0793iHfB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def feature_generator(df, channel_names, epoch_len):\n",
        "    num_epochs = int(len(df) / epoch_len)\n",
        "    all_features = []\n",
        "\n",
        "    for n in range(num_epochs):\n",
        "        for channel in channel_numbers:\n",
        "            data = df.iloc[n * epoch_len:(n + 1) * epoch_len, channel-1]\n",
        "            coeffs = pywt.wavedec(data, wavelet, level = 5) #returns level+1 values, level detail coeffs and 1 approx\n",
        "            \"\"\"Frequency Bands:\n",
        "              The frequency range corresponding to each level is calculated. For a sampling frequency of 250 Hz, the levels correspond roughly to:\n",
        "              Level 1: 62.5 - 125 Hz\n",
        "              Level 2: 31.25 - 62.5 Hz\n",
        "              Level 3: 15.625 - 31.25 Hz\n",
        "              Level 4: 7.8125 - 15.625 Hz\n",
        "              Level 5: 3.9062 - 7.8125 Hz\n",
        "              Approximation: 0 - 3.9062 Hz\n",
        "              The alpha band (8-13 Hz) lies within Level 4.\"\"\"\n",
        "            for i, coeff in enumerate(coeffs[3:], 1):\n",
        "                features = calculate_features(coeff)\n",
        "                all_features.append([channel] + list(features))\n",
        "            features = calculate_features(coeffs[0])\n",
        "            all_features.append([channel] + list(features))\n",
        "\n",
        "    feature_df = pd.DataFrame(all_features, columns=['Channel'] + feature_names)\n",
        "    feature_df.set_index('Channel', inplace=True)\n",
        "\n",
        "    return feature_df"
      ],
      "metadata": {
        "id": "ueiwTyETrH_z"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "dataset labeller"
      ],
      "metadata": {
        "id": "oWMRRYChlCsu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "level_map = [1,2,3,4,5]\n",
        "feature_map = [1,2,3,4]\n",
        "chars = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
        "\n",
        "#num = 20 for label, num = 1 for feature,  num = 4 for level\n",
        "#n = 0 for label, n = 1 for feature, n = 2 for level, in order\n",
        "\n",
        "def drop_column(dataframe, column_name):\n",
        "  try:\n",
        "    df = dataframe.drop(columns=column_name, axis =1)\n",
        "    return df\n",
        "  except Exception as e:\n",
        "    return dataframe\n",
        "\n",
        "def dataset_labeller(dataframe, num_features_per_epoch, chars, n, label):\n",
        "  num_samples = len(dataframe) // num_features_per_epoch\n",
        "  num_labels = len(chars)  # 12 labels\n",
        "\n",
        "  # Create a list of labels for each 32-row segment\n",
        "  labels = []\n",
        "  for i in range(num_samples):\n",
        "      labels.extend([chars[i % num_labels]] * num_features_per_epoch)\n",
        "  # Add the labels to the DataFrame\n",
        "  dataframe.insert(dataframe.shape[1]-n, label , labels, allow_duplicates = False) #df.shape[1] returns the number of columns\n",
        "  return dataframe"
      ],
      "metadata": {
        "id": "72zLhF4IPI29"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "main function"
      ],
      "metadata": {
        "id": "zQAHLWm9Q8PQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing(dataframe, epoch_len):\n",
        "\n",
        "   filtered_df = pd.DataFrame(columns=dataframe.columns)\n",
        "   num_epochs = round(int(len(dataframe.index)/epoch_len))\n",
        "   #print(num_epochs)\n",
        "\n",
        "   for channel, i in enumerate(dataframe.columns):\n",
        "      filtered_col_data = []\n",
        "      for n in range(num_epochs):\n",
        "          data = dataframe.iloc[n*epoch:(n+1)*epoch, channel]\n",
        "          data = notch_filter(data, fs, 50, notch_quality_factor)\n",
        "          data = butter_bandpass_filter(data, lowcut, highcut, fs, order=filter_order)\n",
        "          filtered_col_data.extend(data)\n",
        "      filtered_df[i] = filtered_col_data\n",
        "   feature_df = feature_generator(filtered_df, channel_names, epoch_len) #extract and store features\n",
        "   scaled_df = dataset_scaler(feature_df,  feature_names) #scale dataset AFTER filtering\n",
        "   return scaled_df"
      ],
      "metadata": {
        "id": "sdMgwNFeeZ9J"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dataset_combiner(file_names, pattern):\n",
        "  data = [pd.read_excel(file) for file in file_names]\n",
        "  df = pd.concat(data, axis = 0)\n",
        "  return df"
      ],
      "metadata": {
        "id": "l1JOtXhxfIjt"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dataset_output(file_path):\n",
        "  files = []\n",
        "  for x in os.listdir(file_path):\n",
        "    if x.endswith(\".csv\"):\n",
        "      files.append(x)\n",
        "      files.sort()\n",
        "  for file in files:\n",
        "    df = pd.read_csv(file, header = None)\n",
        "    df = np.transpose(df)\n",
        "    df.columns = channel_names\n",
        "    da = preprocessing(df, 710)\n",
        "    output = da.to_excel(f'MIN{files.index(file)+1}eta.xlsx', sheet_name='sheet1',  float_format = \"%.8f\")\n",
        "  return dataset_combiner(files, \"eta.xlsx\")"
      ],
      "metadata": {
        "id": "bSIVF1hEhuJl"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not drive.mount('/content/drive'):\n",
        "  drive_mounted = True"
      ],
      "metadata": {
        "id": "qqH_Ym6sV36H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(input_method, output_file):\n",
        "  if input_method == 'system':\n",
        "    #dir = get_latest_subdirs('/content')\n",
        "    #input_file = get_input_raw_file(\"/content/OpenBCI-RAW-2024-06-10_14-01-02.txt\")  # Replace with your input file path\n",
        "    fs, header, data = read_input_file(f\"/content/OpenBCI-RAW-2024-06-10_14-01-02.txt\")\n",
        "    #fs, data = convert_and_write_output(output_file,  header, data)\n",
        "    df = pd.DataFrame(data, columns = channel_names)\n",
        "\n",
        "  elif input_method == 'manual':\n",
        "    #Load EEG data from the csv file\n",
        "    df = pd.read_csv(\"S001.csv\", header = None)\n",
        "    #df = pd.read_csv(f\"/content/drive/MyDrive/Final Year Project/Machine Learning/ML Models/Datasets/MINERVA_dataset/S001.csv\", header = None)\n",
        "    df = np.transpose(df)\n",
        "    df.columns = channel_names\n",
        "    num_rows, num_cols = df.shape\n",
        "    num_epochs = int((num_rows)/epoch)\n",
        "\n",
        "  df = preprocessing(df, 710)\n",
        "  print(df.columns)\n",
        "  dataset = flatten(df)\n",
        "  labelled_dataset = dataset_labeller(dataset, 1, targets, 0, 'Label')\n",
        "  output = labelled_dataset.to_excel(f'MINtheta.xlsx', sheet_name='sheet1',  float_format = \"%.8f\")\n",
        "  return labelled_dataset\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    output_file = 'output.csv'  # Replace with your desired output file path\n",
        "    df = main('system', output_file)"
      ],
      "metadata": {
        "id": "lb7_fhJlnqrw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eadf4ee9-2c7c-44da-ed69-4512dfabc3ca"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Mean', 'Standard Deviation', 'Skewness', 'Entropy', 'Energy',\n",
            "       'Kurtosis', 'Median', 'Variance'],\n",
            "      dtype='object')\n",
            "Original data shape: (5664, 8)\n",
            "Flattened data shape: (177, 256)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def flatten(dataframe):\n",
        "  # Define the segment size\n",
        "  segment_height = 32\n",
        "  segment_width = 8\n",
        "  # Calculate the number of segments\n",
        "  num_segments = dataframe.shape[0] // segment_height\n",
        "\n",
        "  # Initialize a list to hold the flattened vectors\n",
        "  flattened_vectors = []\n",
        "\n",
        "  for i in range(num_segments):\n",
        "      # Extract the 8x32 segment\n",
        "      segment = dataframe.iloc[i * segment_height:(i + 1) * segment_height, :]\n",
        "\n",
        "      # Flatten the segment into a 1D vector of length 256\n",
        "      flattened_vector = segment.to_numpy().flatten()\n",
        "\n",
        "      # Add the flattened vector to the list\n",
        "      flattened_vectors.append(flattened_vector)\n",
        "\n",
        "  # Convert the list of vectors to a NumPy array\n",
        "  flattened_vectors = np.array(flattened_vectors)\n",
        "\n",
        "  #print(\"Original data shape:\", dataframe.shape)\n",
        "  #print(\"Flattened data shape:\", flattened_vectors.shape)\n",
        "  da =  pd.DataFrame(flattened_vectors)\n",
        "  return da"
      ],
      "metadata": {
        "id": "6XGs_dkispQz"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "game interface"
      ],
      "metadata": {
        "id": "L2PGsK9s17z1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_output():\n",
        "  model_output = 3\n",
        "  with open(r\"D://Documents\\College\\Semester-VIII\\B.E. Project\\MINERVA_dataset\\Output\\MINERVA_output.txt\", \"w\") as file:\n",
        "    file.write(str(model_output))"
      ],
      "metadata": {
        "id": "lsOf0vZp19pe"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "siamese network"
      ],
      "metadata": {
        "id": "14Er9PHeIVxd"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KHyw3yeTIU1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.spatial import distance\n",
        "\n",
        "arr1 = df.iloc[0,:-1]\n",
        "arr2 = df.iloc[24, :-1]\n",
        "#print(arr1)\n",
        "dist = distance.euclidean(arr1, arr2)\n",
        "print(f'Euclidean distance: {dist}')\n",
        "#THIS TELLS ME THIS DATASET IS A LOAD OF CRAP"
      ],
      "metadata": {
        "id": "McLn5oRizRjZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}